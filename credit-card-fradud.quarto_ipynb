{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Proyecto deteccion de fraude en tarjeta de credito\"\n",
        "author: \"Leandro Soto Miranda\"\n",
        "date: \"2024-10-24\"\n",
        "format: \n",
        "  html: \n",
        "    toc: true \n",
        "    code-fold: true\n",
        "---\n",
        "\n",
        "\n",
        "## 1. Definir problema\n",
        "\n",
        "- En este proyecto, analizaremos un conjunto de datos relacionaciodnado con la productividad\n",
        "\n",
        "## 2. Recopilación de datos\n"
      ],
      "id": "af40e743"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Importación de librerías\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "    \n",
        "# Carga de datos\n",
        "df = pd.read_csv('data.csv', sep=\",\")\n",
        "\n",
        "# Resumen estadístico\n",
        "# Mostrar las primeras filas del dataset\n",
        "print(\"Primeras filas del dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Mostrar información general del dataset\n",
        "print(\"\\nInformación del dataset:\")\n",
        "print(df.info())\n",
        "\n",
        "# Descripción estadística básica del dataset\n",
        "print(\"\\nDescripción estadística:\")\n",
        "print(df.describe())\n",
        "\n",
        "info = df.shape\n",
        "print(\"\\nLa cantidad de filas y columnas en nuestro dataframe es de:\",info)\n",
        "\n",
        "tipos = df.dtypes\n",
        "print(\"\\nTipos de datos presentes en el dataset:\\n\",tipos)"
      ],
      "id": "2348d98a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Análisis de datos por Variable\n",
        "## Análisis de datos cuantitativos\n"
      ],
      "id": "c690a79f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Función para el análisis univariado de variables cuantitativas\n",
        "def analizar_variable_cuantitativa(df, columna):\n",
        "    mean = np.mean(df[columna])\n",
        "    median = np.median(df[columna])\n",
        "    std = np.std(df[columna])\n",
        "    min_value = np.min(df[columna])\n",
        "    max_value = np.max(df[columna])\n",
        "\n",
        "    print(f\"Análisis de la Variable '{columna}'\")\n",
        "    print(f\"Media: {mean:.2f}\")\n",
        "    print(f\"Mediana: {median:.2f}\")\n",
        "    print(f\"Desviación Estándar: {std:.2f}\")\n",
        "    print(f\"Valor Mínimo: {min_value:.2f}\")\n",
        "    print(f\"Valor Máximo: {max_value:.2f}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Visualización de la distribución (Histograma con KDE)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(df[columna], bins=30, kde=True, color='#4C72B0')\n",
        "    plt.title(f'Distribución de {columna}')\n",
        "    plt.xlabel(columna)\n",
        "    plt.ylabel('Frecuencia')\n",
        "    plt.show()\n",
        "\n",
        "# Aplicar la función a todas las columnas numéricas del dataframe\n",
        "columnas_cuantitativas = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "for columna in columnas_cuantitativas:\n",
        "    analizar_variable_cuantitativa(df, columna)"
      ],
      "id": "d0433407",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Análisis de datos cualitativos\n"
      ],
      "id": "f3270241"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Función para el análisis univariado de variables categóricas\n",
        "def analizar_variable_categorica(df, columna):\n",
        "    values_counts = df[columna].value_counts()\n",
        "    moda = values_counts.idxmax()\n",
        "\n",
        "    print(f\"Análisis Univariado de la Variable '{columna}'\")\n",
        "    print(f\"Frecuencia de las categorías:\\n{values_counts}\")\n",
        "    print(f\"Moda (Categoría más frecuente): {moda}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Visualización de la distribución\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.countplot(\n",
        "        x=columna, \n",
        "        data=df[df[columna].isin(values_counts.index)], \n",
        "        order=values_counts.index, \n",
        "        palette='Set2'\n",
        "    )\n",
        "    plt.title(f'Distribución de {columna}')\n",
        "    plt.xlabel(columna)\n",
        "    plt.ylabel('Frecuencia')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# Columnas categóricas a excluir\n",
        "columnas_a_excluir = ['TransactionDate']\n",
        "\n",
        "columnas_categoricas = [col for col in df.select_dtypes(include=['object']).columns if col not in columnas_a_excluir]\n",
        "\n",
        "for columna in columnas_categoricas:\n",
        "    analizar_variable_categorica(df, columna)\n"
      ],
      "id": "265ee523",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verificar si hay patrones o relaciones presentes entre variables\n",
        "\n",
        "## Verificar si hay valores nulos en el dataset\n",
        "### Corrección de valores nulos presentes en el dataset"
      ],
      "id": "33387b5b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\nValores nulos por columna:\")\n",
        "print(df.isnull().sum())"
      ],
      "id": "4a27e660",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Procesamiento de datos\n",
        "# Verificar si hay valores atípicos"
      ],
      "id": "b02f9d93"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def generar_boxplot(df, columna):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.boxplot(data=df, x=columna, palette='Set2')\n",
        "    plt.title(f'Boxplot de {columna}')\n",
        "    plt.xlabel(columna)\n",
        "    plt.show()\n",
        "\n",
        "# Aplicar la función a todas las columnas numéricas del dataframe\n",
        "columnas_numericas = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "for columna in columnas_numericas:\n",
        "    generar_boxplot(df, columna)"
      ],
      "id": "a2f500d8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Para el tratamiento de datos atípicos lo que vamos a realizar es la imputación de ellos mediante el método intercuartílico, pero también vamos a generar un dataset sin realizar ningún tratamiento de los datos outlayers presentes en el dataset para realizar pruebas posteriormente en la aplicación de modelos de machine learning para ver cuanta diferencia hay entre ambos casos."
      ],
      "id": "311120b6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_clean = df.copy()\n",
        "\n",
        "variable_objetivo = 'IsFraud'\n",
        "df_sin_objetivo = df_clean.drop(columns=[variable_objetivo])\n",
        "\n",
        "for col in df_sin_objetivo.select_dtypes(include=['int64', 'float64']).columns:\n",
        "    # Calcula Q1 y Q3\n",
        "    Q1 = df_sin_objetivo[col].quantile(0.25)\n",
        "    Q3 = df_sin_objetivo[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    \n",
        "    # Define los límites inferior y superior\n",
        "    lower_limit = Q1 - 1.5 * IQR\n",
        "    upper_limit = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Imputa los valores atípicos por los límites correspondientes\n",
        "    df_sin_objetivo[col] = np.where(df_sin_objetivo[col] < lower_limit, lower_limit, df_sin_objetivo[col])\n",
        "    df_sin_objetivo[col] = np.where(df_sin_objetivo[col] > upper_limit, upper_limit, df_sin_objetivo[col])\n",
        "\n",
        "# Reinserta la variable objetivo en el DataFrame limpio\n",
        "df_clean = pd.concat([df_sin_objetivo, df_clean[[variable_objetivo]]], axis=1)\n",
        "\n",
        "# Verificamos algunos valores antes y después de la limpieza\n",
        "print(\"Datos antes de la limpieza:\")\n",
        "print(df.describe())\n",
        "print(\"\\nDatos después de la limpieza:\")\n",
        "print(df_clean.describe())"
      ],
      "id": "9d534320",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Lo que realizamos aqui es explir nuestra varialbe objetivo la cual es IsFraud ya que uno de los princip[ales problemas es que al tenere pocos datos para uno de los valores lo clasificaba como outlayer y a la hora de apklciarldo estos datos se eliminaban, si bien son pocos datos lo que se iban eliminando siguen siendo importantes para la posterior balanceo de datos]\n",
        "\n",
        "## Preparación de datos\n",
        "### Vamos a realizar el tratamiento de datos pasos a realizar\n",
        "### Convertir datos categóricos a numéricos para esto vamos a aplicar one hot encoder y label conder dependiendo a tipo de dato\n",
        "### Además de convertir los datos categóricos en general para aplicar estos datos en modelos de machine learning, además de verificar el balanceo de datos y aplicación de técnicas para corregir variables con outlayers\n"
      ],
      "id": "bc8c2d77"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for column in df.columns:\n",
        "    unique_values = df[column].unique()\n",
        "    print(f\"Valores únicos en la columna '{column}':\")\n",
        "    print(unique_values)\n",
        "    print(\"\\n------------------------------------\\n\")"
      ],
      "id": "f9605580",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LabelEncoder\n",
        "\n",
        "### Variables Ninguna\n",
        "\n",
        "## OneHotEncoder\n",
        "\n",
        "### Variables: Location, TransactionType"
      ],
      "id": "1aeb12f9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Aplicar One-Hot Encoding\n",
        "df = pd.get_dummies(df, columns=['Location'])\n",
        "df = pd.get_dummies(df, columns=['TransactionType'])\n",
        "# datos sin outlayers\n",
        "df_clean = pd.get_dummies(df_clean, columns=['Location'])\n",
        "df_clean = pd.get_dummies(df_clean, columns=['TransactionType'])"
      ],
      "id": "1c552e54",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for column in df.columns:\n",
        "    unique_values = df[column].unique()\n",
        "    print(f\"Valores únicos en la columna '{column}':\")\n",
        "    print(unique_values)\n",
        "    print(\"\\n------------------------------------\\n\")"
      ],
      "id": "736b7954",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verificacion sobre el balance de los datos de la variable IsFraud, para posterior tratamiento\n",
        "- Con esto tenemos un problema, nuestra variable objetivo esta demasiado desbalanceada, po lo que tendremos que aplicar balanceo de datos ya bien sea con oversample (SMOTE) a la variable minoritaria o undersample a nuestra variable mayoritaria"
      ],
      "id": "cf85bd1d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "variable = 'IsFraud'\n",
        "balance = df[variable].value_counts(normalize=True) * 100\n",
        "\n",
        "print(f\"Distribución de '{variable}':\")\n",
        "print(balance)\n",
        "\n",
        "# Visualizar distribución con un gráfico de barras\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(data=df, x=variable, order=balance.index, palette=\"viridis\")\n",
        "plt.title(f\"Distribución de la variable '{variable}'\")\n",
        "plt.xlabel(variable)\n",
        "plt.ylabel(\"Frecuencia\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "id": "0cd06946",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Aplicación de Class Weighting en el Modelo"
      ],
      "id": "b3faa3e9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Dividir en variables predictoras (X) y la variable objetivo (y)\n",
        "X = df.drop(columns=[\"IsFraud\", \"TransactionDate\", \"TransactionType\",\"Location\"])\n",
        "y = df[\"IsFraud\"]\n",
        "\n",
        "# Dividir en conjunto de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Aplicación de class_weight para un modelo (ejemplo con RandomForest)\n",
        "model_rf = RandomForestClassifier(class_weight=\"balanced\", random_state=42)\n",
        "model_rf.fit(X_train, y_train)\n",
        "\n",
        "# Otro ejemplo con Logistic Regression\n",
        "model_lr = LogisticRegression(class_weight=\"balanced\", random_state=42)\n",
        "model_lr.fit(X_train, y_train)"
      ],
      "id": "b28cce08",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combination of Oversampling (con RandomOverSampler) y Class Weighting\n"
      ],
      "id": "d2169dc5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Crear un oversampler\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
        "\n",
        "# Crear un nuevo DataFrame con el dataset balanceado\n",
        "df_balanced = pd.DataFrame(X_resampled, columns=X.columns)\n",
        "df_balanced[\"IsFraud\"] = y_resampled\n",
        "\n",
        "# Calcular class_weight para el modelo\n",
        "class_weights = compute_class_weight(\"balanced\", classes=np.array([0, 1]), y=y_resampled)\n",
        "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "\n",
        "# Modelo con class_weight aplicado\n",
        "model_balanced_rf = RandomForestClassifier(class_weight=class_weight_dict, random_state=42)\n",
        "model_balanced_rf.fit(X_resampled, y_resampled)\n",
        "\n",
        "print(\"Distribución después del balanceo:\")\n",
        "print(df_balanced[\"IsFraud\"].value_counts())"
      ],
      "id": "2596b15f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "variable = 'IsFraud'\n",
        "balance = df_balanced[variable].value_counts(normalize=True) * 100\n",
        "\n",
        "print(f\"Distribución de '{variable}':\")\n",
        "print(balance)\n",
        "\n",
        "# Visualizar distribución con un gráfico de barras\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(data=df_balanced, x=variable, order=balance.index, palette=\"viridis\")\n",
        "plt.title(f\"Distribución de la variable '{variable}'\")\n",
        "plt.xlabel(variable)\n",
        "plt.ylabel(\"Frecuencia\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "id": "58c2e42f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calcular la matriz de correlación\n",
        "corr_matrix = df_balanced.corr()\n",
        "\n",
        "# Generar el mapa de calor\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "plt.title('Mapa de calor de la correlación entre variables')\n",
        "plt.show()"
      ],
      "id": "b1fb98e2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calcular la matriz de correlación\n",
        "correlation_matrix = df_balanced.corr()\n",
        "\n",
        "# Filtrar solo las correlaciones con la variable objetivo\n",
        "target_corr = correlation_matrix['IsFraud'].sort_values(ascending=False)\n",
        "print(\"Correlación de cada variable con 'Productivity_Score':\\n\", target_corr)"
      ],
      "id": "baf85c4e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Aplicación de modelos de machine learning\n",
        "\n",
        "## Predicción con datos atípicos\n"
      ],
      "id": "a28b5040"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Variables utilizadas\n",
        "print(\"Variables utilizadas en el modelo:\")\n",
        "print(X.columns)"
      ],
      "id": "8217e69a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predicción sin datos atípicos\n"
      ],
      "id": "47e447fc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Seleccionar características (features) y la variable objetivo (target)\n",
        "X = df_balanced.drop(columns=['IsFraud'])\n",
        "y = df_balanced['IsFraud']\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "id": "ec5b2644",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Crear el modelo de Regresión Logística\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predecir en los datos de prueba\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluar el modelo\n",
        "accuracy_RL = accuracy_score(y_test, y_pred)\n",
        "matrix_RL = confusion_matrix(y_test, y_pred)\n",
        "reporte_RL = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f'Precisión del modelo: {accuracy_RL:.2f}')\n",
        "print('Matriz de confusión:')\n",
        "print(matrix_RL)\n",
        "print('Reporte de clasificación:')\n",
        "print(reporte_RL)"
      ],
      "id": "d9a3037d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import joblib\n",
        "\n",
        "# Crear el modelo de Árbol de Decisión\n",
        "model = DecisionTreeClassifier()\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predecir en los datos de prueba\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluar el modelo\n",
        "accuracy_AD = accuracy_score(y_test, y_pred)\n",
        "matrix_AD = confusion_matrix(y_test, y_pred)\n",
        "reporte_AD = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f'Precisión del modelo: {accuracy_AD:.2f}')\n",
        "print('Matriz de confusión:')\n",
        "print(matrix_AD)\n",
        "print('Reporte de clasificación:')\n",
        "print(reporte_AD)"
      ],
      "id": "e8081e2c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Crear el modelo de Support Vector Machine\n",
        "model = SVC()\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predecir en los datos de prueba\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluar el modelo\n",
        "accuracy_SVM = accuracy_score(y_test, y_pred)\n",
        "matrix_SVM = confusion_matrix(y_test, y_pred)\n",
        "reporte_SVM = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f'Precisión del modelo: {accuracy_SVM:.2f}')\n",
        "print('Matriz de confusión:')\n",
        "print(matrix_SVM)\n",
        "print('Reporte de clasificación:')\n",
        "print(reporte_SVM)"
      ],
      "id": "7b0e6d6c",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\lea\\AppData\\Roaming\\Python\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}